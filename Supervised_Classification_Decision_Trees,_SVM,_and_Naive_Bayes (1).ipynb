{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "Answer:-Information Gain measures how much uncertainty (impurity) is reduced after splitting the data using a feature.\n",
        "\n",
        "A Decision Tree always tries to make the data as pure as possible, meaning\n",
        "\n",
        "**How is it used in Decision Trees**\n",
        "\n",
        "1. Calculate entropy of the parent node\n",
        "\n",
        "2.  Split the data using each feature\n",
        "\n",
        "3.  Calculate entropy after each split\n",
        "\n",
        "4.  Compute Information Gain\n",
        "\n",
        "5.  Choose the feature with the highest Information Gain"
      ],
      "metadata": {
        "id": "UX7h10BvVae1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "Answer:-**Gini Impurity** measures the probability that a randomly chosen data point would be incorrectly classified if it were labeled according to the class distribution of the node. It is computationally faster because it does not involve logarithmic calculations. Gini Impurity is mainly used in the CART algorithm and works well for large datasets.\n",
        "\n",
        "**Entropy** measures the amount of uncertainty or randomness in the data. It is based on information theory and uses logarithmic calculations. Entropy is more sensitive to changes in class probabilities and is used in algorithms like ID3 and C4.5 through Information Gain.\n",
        "\n",
        "In practice, both measures often produce similar results. Gini Impurity is preferred when speed is important, while Entropy is chosen when a more informative and theoretically grounded measure is required.\n"
      ],
      "metadata": {
        "id": "GwDQO94_KnbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "Answer:-Pre-Pruning is a technique used in Decision Trees to stop the tree from growing too deep during training. The main purpose of pre-pruning is to prevent overfitting and improve the model’s ability to generalize to new, unseen data.\n",
        "\n",
        "In pre-pruning, the algorithm applies stopping criteria while building the tree, such as:\n",
        "\n",
        "-  Maximum depth of the tree\n",
        "\n",
        "-  Minimum number of samples required to split a node\n",
        "\n",
        "-  Minimum Information Gain or Gini reduction\n",
        "\n",
        "-  Maximum number of leaf nodes\n"
      ],
      "metadata": {
        "id": "OtfucVObLXz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 4:Write a Python program to train a Decision Tree Classifier using Gini Imurity as the criterion and print the feature importances (practical).**\n"
      ],
      "metadata": {
        "id": "TIw2qymlL1ij"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZZS5OPKuGDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca62b276-d27f-4e50-dd52-7cafd38db08f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "[1. 0.]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "# Features: [Age, Income]\n",
        "X = np.array([\n",
        "    [25, 40000],\n",
        "    [30, 50000],\n",
        "    [45, 80000],\n",
        "    [35, 65000],\n",
        "    [22, 30000],\n",
        "    [50, 90000]\n",
        "])\n",
        "\n",
        "# Target labels (0 = No, 1 = Yes)\n",
        "y = np.array([0, 0, 1, 1, 0, 1])\n",
        "\n",
        "# Create Decision Tree model with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "print(model.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "Answer:-A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its main objective is to find an optimal hyperplane that separates data points of different classes with the maximum margin.\n",
        "\n",
        "SVM focuses on the data points that lie closest to the decision boundary, known as support vectors. These support vectors are the most important points because they directly influence the position and orientation of the hyperplane.\n",
        "\n",
        "SVM can handle both linearly separable and non-linearly separable data. For non-linear data, it uses kernel functions (such as linear, polynomial, and RBF) to transform the data into a higher-dimensional space where separation becomes possible.\n"
      ],
      "metadata": {
        "id": "JPVfwPoCMeQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 6: What is the Kernel Trick in SVM?**\n",
        "Answer:-The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data. Instead of explicitly transforming the data into a higher-dimensional space, the kernel trick allows SVM to compute inner products in that space directly, making the computation efficient.\n",
        "\n",
        "In simple terms, the kernel trick enables SVM to draw a non-linear decision boundary in the original feature space by implicitly mapping data to a higher dimension where the classes become linearly separable.\n",
        "\n",
        "Commonly used kernel functions include:\n",
        "\n",
        "-  Linear Kernel – used when data is linearly separable\n",
        "\n",
        "-  Polynomial Kernel – captures polynomial relationships\n",
        "\n",
        "-  RBF Kernel – handles complex, non-linear patterns\n",
        "\n",
        "-  Sigmoid Kernel – similar to neural networks"
      ],
      "metadata": {
        "id": "VuTlUATvMvHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "ozblS1NVNNu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with Linear and RBF kernels on Wine dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear Kernel Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_S7K3-pPRV6",
        "outputId": "3d12dc9b-9535-45f2-ee61-a4d7f9882839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "Answer:-The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes’ Theorem. It is mainly used for classification tasks, especially in text classification, spam detection, and sentiment analysis.\n",
        "\n",
        "Naïve Bayes calculates the probability of a class given the input features and predicts the class with the highest posterior probability.\n",
        "\n",
        "It is called “Naïve” because it makes a strong assumption that all features are conditionally independent of each other given the class label. In real-world data, this assumption is usually not true, but the algorithm still performs well in many cases.\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "-  Based on Bayes’ Theorem\n",
        "\n",
        "-  Assumes feature independence\n",
        "\n",
        "-  Simple, fast, and efficient\n",
        "\n",
        "-  Works well with large datasets"
      ],
      "metadata": {
        "id": "-W8MSXlAPdCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "Answer:-**1.Gaussian Naïve Bayes**\n",
        "\n",
        "-  **Features:** Continuous (numeric)\n",
        "\n",
        "-  **Assumption:** Features follow normal (Gaussian) distribution\n",
        "\n",
        "-  **Use Case:** Height, weight, blood pressure, etc.\n",
        "\n",
        "**2.Multinomial Naïve Bayes**\n",
        "\n",
        "-  **Features:** Discrete counts\n",
        "\n",
        "-  **Assumption:** Works on frequency of features\n",
        "\n",
        "-  **Use Case:** Text classification, spam detection (word counts)\n",
        "\n",
        "**3.Bernoulli Naïve Bayes**\n",
        "\n",
        "-  **Features:** Binary (0 or 1)\n",
        "\n",
        "-  **Assumption:** Models presence or absence of features\n",
        "\n",
        "-  **Use Case:** Spam detection, text with binary word occurrence"
      ],
      "metadata": {
        "id": "YVdO9U5iP1C5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 10:Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.**\n",
        "Answer:-\n"
      ],
      "metadata": {
        "id": "FmfgwG5qRTOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Naïve Bayes on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy of Gaussian Naive Bayes:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE1dEuE2RUJk",
        "outputId": "b2cf18e6-f4b8-4b2d-c5a6-8e7ff6ac3b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8W-zxcrRgH7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}